## MTL

### Deep Multi-Task Learning – 3 Lessons Learned

![img](D:\gitrep\ml_theory\ml\多任务.assets\mtl-300x248.png)



主要阐述两个问题：

1.多目标学习任务的loss 相加，会导致一些问题，比如任务的loss 是不是一个数量级别的，如果不是一个数量级别的任务放在一起，就会导致任务的训练的偏移，举个最简单的例子，我们将0.1->0.11 ，100 两个同规模的任务，100任务的loss 怼shared layers 占主导地位，导致100的任务训练比较好，而0.11的训练任务会比较差，简单的相加可以应对loss 在同一个数量级别，比如任务都是两分类，判断图片里的任务是不是微笑，是不是有胡子，男女等这类任务，因为其对share layers 的影响相对较小（可做个实验）。方案是：加权（多个超参数） 或者引入噪声的方式[paper](https://arxiv.org/abs/1705.07115)，论文中基本上看是对不同的loss 的权重进行了参数化，可以自动学出[example](https://github.com/yaringal/multi-task-learning-example)

[mmoe](https://zhuanlan.zhihu.com/p/55752344?edition=yidianzixun&utm_source=yidianzixun&yidian_docid=0LC8kTgk) 也间接的解决了这个问题

2.学习率的调整，多个任务使用同一个学习率的是时候，会导致一些任务dying relu ,而调整小之后又导致训练缓慢。所以对不同的子任务采用不同的学习率的方式进行解决

**分类：**

1. 基于特征学习的MTL

   - 基于特征转换的MTL
   - 基于特征选择的MTL

   <img src="D:\gitrep\ml_theory\ml\多任务.assets\v2-494b6bc474d62430381259fee9ac9245_1440w.jpg" alt="img" style="zoom:50%;" />

2. 基于低秩的MTL

3. 基于任务聚类的MTL

4. 基于任务关系学习的MTL

5. 基于分解的MTL

**归纳偏置**

![img](D:\gitrep\ml_theory\ml\多任务.assets\v2-e544a36bf82fa3e20e233e0e5de06b43_720w.jpg)



#### Reference

- [Deep Multi-Task Learning – 3 Lessons Learned](https://engineering.taboola.com/deep-multi-task-learning-3-lessons-learned/)
- [多任务学习](https://blog.csdn.net/xuluohongshang/article/details/79044325)
- [概念学习和归纳偏置](https://blog.51cto.com/underthehood/590838)
- [多任务学习综述-A Survey on Multi-Task Learning](https://zhuanlan.zhihu.com/p/67524006)
- [MMOE 简介](https://zhuanlan.zhihu.com/p/55752344?edition=yidianzixun&utm_source=yidianzixun&yidian_docid=0LC8kTgk)
- [git code mmoe](https://github.com/drawbridge/keras-mmoe)
- [Multi-task Learning(Review)多任务学习概述](https://zhuanlan.zhihu.com/p/59413549)